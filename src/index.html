<!doctype html>
<html lang="en">

<head>
    <title>LLMs Process Lists With General Filter Heads</title>
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="description"
        content="LLMs learn to implement a neural analogue of filtering operations similar to functional programming with specialized attention heads" />
    <meta property="og:title" content="LLMs Process Lists With General Filter Heads" />
    <meta property="og:url" content="https://filter.baulab.info/" />
    <meta property="og:image" content="https://filter.baulab.info/images/lre-thumb.png" />
    <meta property="og:description"
        content="LLMs learn to implement a neural analogue of filtering operations similar to functional programming with specialized attention heads" />
    <meta property="og:type" content="website" />
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="LLMs Process Lists With General Filter Heads" />
    <meta name="twitter:description"
        content="LLMs learn to implement a neural analogue of filtering operations similar to functional programming with specialized attention heads" />
    <meta name="twitter:image" content="https://filter.baulab.info/images/lre-thumb.png" />
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">

    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
    <script src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+Math&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
    <link href="style.css" rel="stylesheet">

    <style>
        .relatedthumb {
            float: left;
            width: 200px;
            margin: 3px 10px 7px 0;
        }

        .relatedblock {
            clear: both;
            display: inline-block;
        }

        .bold-sc {
            font-variant: small-caps;
            font-weight: bold;
        }

        .cite,
        .citegroup {
            margin-bottom: 8px;
        }

        :target {
            background-color: yellow;
        }
    </style>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-FD12LWN557"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date()); gtag('config', 'G-FD12LWN557');
    </script>

</head>

<body class="nd-docs">
    <div class="nd-pageheader">
        <div class="container">
            <h1 class="lead">
                <nobr class="widenobr">LLMs Process Lists With General Filter Heads</nobr>
            </h1>
            <address>
                <nobr><a href="https://arnab-api.github.io/" target="_blank">Arnab Sen Sharma</a>,</nobr>
                <nobr><a href="https://github.com/giordanorogers" target="_blank">Giordano Rogers</a>,
                </nobr>
                <nobr><a href="http://scholar.google.co.il/citations?hl=en&user=hd7quH4AAAAJ&sortby=pubdate"
                        target="_blank">Natalie Shapira</a>,</nobr>
                <nobr><a href="https://baulab.info/" target="_blank">David Bau</a></nobr>
                <br>
                <nobr><a href="https://khoury.northeastern.edu/" target="_blank">Northeastern
                        University</a>,</nobr>
            </address>
        </div>
    </div><!-- end nd-pageheader -->

    <div class="container">
        <div class="row justify-content-center" style="margin-bottom: 20px">
            <!-- <p class="text-center">
<a href="https://lre.baulab.us/"
   >New!  Try interacting with a lre-edited GPT to see the effect of inserting hundreds of memories.</a>
</p> -->
        </div>
        <div class="row justify-content-center text-center">

            <p>
                <a href="https://arxiv.org/pdf/2308.09124.pdf" class="d-inline-block p-3 align-top" target="_blank">
                    <img height="100" width="78" src="images/paper-thumb.png" style="border:1px solid; margin: 0 38px;"
                        alt="ArXiv Preprint thumbnail" data-nothumb="">
                    <br>ArXiv<br>Preprint</a>
                <a href="https://github.com/arnab-api/filter" class="d-inline-block p-3 align-top" target="_blank">
                    <img height="100" width="78" src="images/code-thumb.png" style="border:1px solid; margin: 0 38px;"
                        alt="Github code thumbnail" data-nothumb="">
                    <br>Source Code<br>
                </a>
            </p>

            <div class="card" style="max-width: 1020px;">
                <div class="card-block">
                    <h3>How do Transformer LMs filter from a list of candidates?</h3>
                    <p style="text-align: justify;">
                        When LLMs are asked to perform a filtering operation such as <i>find the fruit</i> in a list,
                        they use systematic mechanisms surprisingly similar to certain functional programming patterns.
                    <p style="text-align: justify;">
                        We find that LLMs implement a neural analogue of filtering operations using specialized
                        attention heads
                        that we call <b style="font-family:'Times New Roman'; font-size:19px">filter heads</b>. These
                        heads encode the filtering criterion (the <i>predicate</i>) in their query states of certain
                        tokens. This encoding is sufficiently abstract that it can be transported to a different context
                        to trigger the execution of the same filtering operation on a new list of candidates, presented
                        in a different format/language, even in a
                        different task.
                    </p>
                </div>
            </div>

        </div>

        <div class="row">
            <div class="col">
                <figure class="center_image" style="margin-top: 30px">
                    <img src="images/Paper/filter_head_patching.png" style="width:100%">
                    <figcaption>
                        A filter head [35, 19] in Llama-70B encodes a compact representation
                        of the predicate <em>"is this fruit?"</em>.
                        <strong>(a)</strong> Within a prompt p<sub>src</sub> to find a fruit in a list, we examine the
                        attention head's behavior at the last token <code>":"</code>.
                        <strong>(b)</strong> The head focuses its attention on the one fruit in the list.
                        <strong>(c)</strong> We examine the same attention head's behavior in a second prompt
                        p<sub>dest</sub> searching a different list for a vehicle.
                        <strong>(d)</strong> and we also examine the behavior of the head when patching its query state
                        to use the q<sub>src</sub> vector from the source context.
                        <strong>(e)</strong> The head attends to the vehicle but then
                        <strong>(f)</strong> redirects its attention to the fruit in the new list after the query vector
                        is patched.
                        <strong>(g)</strong> A sparse set of attention heads work together to conduct filtering over a
                        wide range of predicates. These filter heads are concentrated in the middle layers (out of 80
                        layers in Llama-70B).
                    </figcaption>
                </figure>



                <h2>Citation</h2>

                <p>This work is under review. The preprint can be cited as follows.
                </p>

                <div class="card">
                    <h3 class="card-header">bibliography</h3>
                    <div class="card-block">
                        <p style="text-indent: -3em; margin-left: 3em;" class="card-text clickselect">
                            Arnab Sen Sharma, Giordano Rogers, Natalie Shapira, and David Bau. "<em>LLMs Process Lists
                                With General Filter Heads</em>" (2025). arXiv preprint.
                        </p>
                    </div>
                    <h3 class="card-header">bibtex</h3>
                    <div class="card-block">
                        <pre class="card-text clickselect">
@article{sensharma2023filter,
    title={LLMs Process Lists With General Filter Heads}, 
    author={Arnab Sen Sharma and Giordano Rogers and Natalie Shapira and David Bau},
    year={2025},
    eprint={},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
</pre>
                    </div>
                </div>
                </p>

            </div>
        </div><!--row -->
    </div> <!-- container -->

    <footer class="nd-pagefooter">
        <div class="row">
            <div class="col-6 col-md text-center">
                <a href="https://baulab.info/">About the Bau Lab</a>
            </div>
        </div>
    </footer>

</body>
<script>
    $(document).on('click', '.clickselect', function (ev) {
        var range = document.createRange();
        range.selectNodeContents(this);
        var sel = window.getSelection();
        sel.removeAllRanges();
        sel.addRange(range);
    });
</script>

</html>